{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjdrnqja9/TIL/blob/main/deep_learning/07_02_gradient_descent_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWfUcmPxxZCJ"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fb-L6g3DxZCM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Iz-oMeI9xZCN"
      },
      "outputs": [],
      "source": [
        "target = torch.FloatTensor([[.1, .2, .3],\n",
        "                            [.4, .5, .6],\n",
        "                            [.7, .8, .9]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vPwf_YYxZCO",
        "outputId": "b8a086d8-4273-4925-d199-f661cfdcbd03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0698, 0.3586, 0.2421],\n",
              "        [0.8683, 0.3019, 0.6978],\n",
              "        [0.7625, 0.0019, 0.5318]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x = torch.rand_like(target)\n",
        "# This means the final scalar will be differentiate by x.\n",
        "x.requires_grad = True\n",
        "# You can get gradient of x, after differentiation.\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJVXH3c5xZCO",
        "outputId": "938fe566-011e-4825-c09b-f06749624d7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1193, grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "loss = F.mse_loss(x, target)\n",
        "\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YDwXU_fxZCP",
        "outputId": "aea9dc2e-e33b-4765-94a7-2e4699cdda7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-th Loss: 7.2191e-02\n",
            "tensor([[0.0765, 0.3233, 0.2550],\n",
            "        [0.7643, 0.3459, 0.6760],\n",
            "        [0.7486, 0.1792, 0.6136]], requires_grad=True)\n",
            "2-th Loss: 4.3671e-02\n",
            "tensor([[0.0817, 0.2959, 0.2650],\n",
            "        [0.6833, 0.3802, 0.6591],\n",
            "        [0.7378, 0.3172, 0.6773]], requires_grad=True)\n",
            "3-th Loss: 2.6418e-02\n",
            "tensor([[0.0858, 0.2746, 0.2728],\n",
            "        [0.6204, 0.4068, 0.6460],\n",
            "        [0.7294, 0.4245, 0.7268]], requires_grad=True)\n",
            "4-th Loss: 1.5981e-02\n",
            "tensor([[0.0889, 0.2580, 0.2788],\n",
            "        [0.5714, 0.4275, 0.6358],\n",
            "        [0.7229, 0.5079, 0.7653]], requires_grad=True)\n",
            "5-th Loss: 9.6677e-03\n",
            "tensor([[0.0914, 0.2451, 0.2835],\n",
            "        [0.5333, 0.4436, 0.6278],\n",
            "        [0.7178, 0.5728, 0.7952]], requires_grad=True)\n",
            "6-th Loss: 5.8484e-03\n",
            "tensor([[0.0933, 0.2351, 0.2872],\n",
            "        [0.5037, 0.4561, 0.6216],\n",
            "        [0.7138, 0.6233, 0.8185]], requires_grad=True)\n",
            "7-th Loss: 3.5379e-03\n",
            "tensor([[0.0948, 0.2273, 0.2900],\n",
            "        [0.4806, 0.4659, 0.6168],\n",
            "        [0.7108, 0.6626, 0.8366]], requires_grad=True)\n",
            "8-th Loss: 2.1402e-03\n",
            "tensor([[0.0960, 0.2212, 0.2923],\n",
            "        [0.4627, 0.4735, 0.6131],\n",
            "        [0.7084, 0.6931, 0.8507]], requires_grad=True)\n",
            "9-th Loss: 1.2947e-03\n",
            "tensor([[0.0969, 0.2165, 0.2940],\n",
            "        [0.4488, 0.4794, 0.6102],\n",
            "        [0.7065, 0.7169, 0.8616]], requires_grad=True)\n",
            "10-th Loss: 7.8321e-04\n",
            "tensor([[0.0976, 0.2128, 0.2953],\n",
            "        [0.4379, 0.4840, 0.6079],\n",
            "        [0.7051, 0.7353, 0.8702]], requires_grad=True)\n",
            "11-th Loss: 4.7379e-04\n",
            "tensor([[0.0981, 0.2100, 0.2964],\n",
            "        [0.4295, 0.4875, 0.6062],\n",
            "        [0.7039, 0.7497, 0.8768]], requires_grad=True)\n",
            "12-th Loss: 2.8662e-04\n",
            "tensor([[0.0985, 0.2078, 0.2972],\n",
            "        [0.4230, 0.4903, 0.6048],\n",
            "        [0.7031, 0.7609, 0.8820]], requires_grad=True)\n",
            "13-th Loss: 1.7339e-04\n",
            "tensor([[0.0988, 0.2060, 0.2978],\n",
            "        [0.4179, 0.4924, 0.6037],\n",
            "        [0.7024, 0.7696, 0.8860]], requires_grad=True)\n",
            "14-th Loss: 1.0489e-04\n",
            "tensor([[0.0991, 0.2047, 0.2983],\n",
            "        [0.4139, 0.4941, 0.6029],\n",
            "        [0.7019, 0.7763, 0.8891]], requires_grad=True)\n",
            "15-th Loss: 6.3450e-05\n",
            "tensor([[0.0993, 0.2037, 0.2987],\n",
            "        [0.4108, 0.4954, 0.6023],\n",
            "        [0.7014, 0.7816, 0.8915]], requires_grad=True)\n",
            "16-th Loss: 3.8384e-05\n",
            "tensor([[0.0995, 0.2028, 0.2990],\n",
            "        [0.4084, 0.4964, 0.6018],\n",
            "        [0.7011, 0.7857, 0.8934]], requires_grad=True)\n",
            "17-th Loss: 2.3220e-05\n",
            "tensor([[0.0996, 0.2022, 0.2992],\n",
            "        [0.4065, 0.4972, 0.6014],\n",
            "        [0.7009, 0.7889, 0.8949]], requires_grad=True)\n",
            "18-th Loss: 1.4047e-05\n",
            "tensor([[0.0997, 0.2017, 0.2994],\n",
            "        [0.4051, 0.4979, 0.6011],\n",
            "        [0.7007, 0.7913, 0.8960]], requires_grad=True)\n",
            "19-th Loss: 8.4973e-06\n",
            "tensor([[0.0997, 0.2013, 0.2995],\n",
            "        [0.4040, 0.4983, 0.6008],\n",
            "        [0.7005, 0.7933, 0.8969]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "threshold = 1e-5\n",
        "learning_rate = 1.\n",
        "iter_cnt = 0\n",
        "\n",
        "while loss > threshold:\n",
        "    iter_cnt += 1\n",
        "    \n",
        "    loss.backward() # Calculate gradients.\n",
        "\n",
        "    x = x - learning_rate * x.grad\n",
        "    \n",
        "    # You don't need to aware this now.\n",
        "    x.detach_()\n",
        "    x.requires_grad_(True)\n",
        "    \n",
        "    loss = F.mse_loss(x, target)\n",
        "    \n",
        "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2LUOsAhxZCP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "07-02-gradient_descent-checkpoint.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}