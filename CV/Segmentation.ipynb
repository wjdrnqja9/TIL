{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjdrnqja9/TIL/blob/main/CV/Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a885f428",
      "metadata": {
        "id": "a885f428"
      },
      "source": [
        "# CT 복원영상에서 폐, 기도, 심장을 가이드해주는 딥러닝 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782cf237",
      "metadata": {
        "id": "782cf237"
      },
      "source": [
        "## 1. CT이미지 데이터셋 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827b16e7",
      "metadata": {
        "id": "827b16e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2dd353",
      "metadata": {
        "id": "5c2dd353"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./DATASET/Segmentation/\"\n",
        "data_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c265ea",
      "metadata": {
        "id": "e6c265ea"
      },
      "outputs": [],
      "source": [
        "def extract_client_id(x):\n",
        "    return x.split(\"_\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9575cb",
      "metadata": {
        "id": "3a9575cb"
      },
      "outputs": [],
      "source": [
        "data_df[\"Id\"] = data_df.ImageId.apply(lambda x:extract_client_id(x))\n",
        "data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93535a3c",
      "metadata": {
        "id": "93535a3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccfa554",
      "metadata": {
        "id": "3ccfa554"
      },
      "outputs": [],
      "source": [
        "client_ids = np.unique(data_df.Id.values)\n",
        "print(len(client_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a9750f",
      "metadata": {
        "id": "49a9750f"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "client_data = data_df[data_df.Id == client_ids[index]][[\"ImageId\", \"MaskId\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1439e1e",
      "metadata": {
        "id": "f1439e1e"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9093b00d",
      "metadata": {
        "id": "9093b00d"
      },
      "outputs": [],
      "source": [
        "# segmentation 시 png, array형태로 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a325b0",
      "metadata": {
        "id": "18a325b0"
      },
      "outputs": [],
      "source": [
        "sub_index = 16\n",
        "img_name, mask_name = client_data.iloc[sub_index].values\n",
        "img_path, mask_path = os.path.join(data_dir, \"images\", img_name), os.path.join(data_dir, \"masks\", mask_name)\n",
        "image = cv2.imread(img_path)\n",
        "mask = cv2.imread(mask_path)\n",
        "thres = 240\n",
        "mask[mask < thres] = 0\n",
        "mask[mask >= thres] = 255\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.subplot(121)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image)\n",
        "plt.subplot(122)\n",
        "plt.title(\"mask\")\n",
        "plt.imshow(mask)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f3951d",
      "metadata": {
        "id": "04f3951d"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask[...,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65183dfb",
      "metadata": {
        "id": "65183dfb"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask[...,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e8dbd9",
      "metadata": {
        "id": "62e8dbd9"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask[...,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676d4b61",
      "metadata": {
        "id": "676d4b61"
      },
      "outputs": [],
      "source": [
        "def get_client_data(data_df, index):\n",
        "    mbclient_ids = np.unique(data_df.Id.values)\n",
        "    client_id = client_ids[index]\n",
        "    client_data = data_df[data_df.Id == client_id]\n",
        "    image_files = list(client_data[\"ImageId\"])\n",
        "    mask_files = list(client_data[\"MaskId\"])\n",
        "    return client_id, image_files, mask_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ed8161",
      "metadata": {
        "id": "b6ed8161"
      },
      "outputs": [],
      "source": [
        "regions = [\"background\", \"trachea\", \"heart\", \"lung\"]\n",
        "colors = ((0,0,0), (255, 0, 0), (0, 255, 0), (0, 0, 255))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3dcf3d",
      "metadata": {
        "id": "3f3dcf3d"
      },
      "outputs": [],
      "source": [
        "index = 1\n",
        "client_id, image_files, mask_files = get_client_data(data_df, index)\n",
        "\n",
        "canvas = np.zeros(shape=(512, 2*512+50, 3), dtype=np.uint8) # 폭 마진\n",
        "\n",
        "for i in range(len(image_files)):\n",
        "    image = cv2.imread(os.path.join(data_dir, \"images\", image_files[i]))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.imread(os.path.join(data_dir, \"masks\", mask_files[i]))\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "    thres = 240\n",
        "    mask[mask < thres] = 0\n",
        "    mask[mask >= thres] = 255\n",
        "    grid_pad = 50\n",
        "    \n",
        "    canvas[:, :512, :] = image\n",
        "    canvas[:, 512+grid_pad:2*512+grid_pad, :] = mask\n",
        "    \n",
        "    text_buff = 410\n",
        "    for j in range(1, len(regions)):\n",
        "        cv2.putText(canvas, f'{regions[j].upper()}', (900, text_buff), cv2.FONT_HERSHEY_SIMPLEX, 1, colors[j], 2)\n",
        "        text_buff += 40\n",
        "    \n",
        "    cv2.imshow('CT frames', canvas)\n",
        "    key = cv2.waitKey(60)\n",
        "    if key == 27:\n",
        "        break\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389a654e",
      "metadata": {
        "id": "389a654e"
      },
      "source": [
        "## 2. 데이터셋 구축과 연산을 위한 텐서변환 모듈 작성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c05978",
      "metadata": {
        "id": "e4c05978"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88480c5b",
      "metadata": {
        "id": "88480c5b"
      },
      "outputs": [],
      "source": [
        "class CT_dataset():\n",
        "    def __init__(self, data_dir, phase, transformer=None): # 클래스가 호출되어 인스턴스가 생성될 때 호출자\n",
        "        self.phase = phase\n",
        "        self.images_dir = os.path.join(data_dir, phase, \"images\")\n",
        "        self.masks_dir = os.path.join(data_dir, phase, \"masks\")\n",
        "        self.image_files = [filename for filename in os.listdir(self.images_dir) if filename.endswith(\"jpg\")]\n",
        "        self.mask_files = [filename for filename in os.listdir(self.masks_dir) if filename.endswith(\"jpg\")]\n",
        "        assert len(self.image_files) == len(self.mask_files)\n",
        "        \n",
        "        self.transformer = transformer\n",
        "    \n",
        "    def __len__(self): # 데이터셋의 수\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def __getitem__(self, index): # 특정 index의 데이터 이미지 라벨 값\n",
        "        image = cv2.imread(os.path.join(self.images_dir, self.image_files[index]))\n",
        "        image = cv2.resize(image, dsize=(IMAGE_SIZE,IMAGE_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "        mask = cv2.imread(os.path.join(self.masks_dir, self.mask_files[index]))\n",
        "        mask = cv2.resize(mask, dsize=(IMAGE_SIZE,IMAGE_SIZE), interpolation=cv2.INTER_NEAREST) \n",
        "        \n",
        "        mask[mask < 240] = 0\n",
        "        mask[mask >= 240] = 255 # 이미 픽셀이 뭉개진 값 처리\n",
        "        mask = mask / 255 # 노멀라이즈\n",
        "        \n",
        "        mask_H, mask_W, mask_C = mask.shape\n",
        "        background = np.ones(shape=(mask_H, mask_W))\n",
        "        background[mask[...,0] != 0] = 0\n",
        "        background[mask[...,1] != 0] = 0\n",
        "        background[mask[...,2] != 0] = 0\n",
        "        mask = np.concatenate([np.expand_dims(background, -1), mask], axis=-1) # 백그라운드 채널 익스펜드\n",
        "        mask = np.argmax(mask, axis=-1, keepdims=False) \n",
        "        \n",
        "        if self.transformer:\n",
        "            image = self.transformer(image)\n",
        "        target = torch.from_numpy(mask).long()\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445e3f3f",
      "metadata": {
        "id": "445e3f3f"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f2bd976",
      "metadata": {
        "id": "7f2bd976"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867aa168",
      "metadata": {
        "id": "867aa168"
      },
      "outputs": [],
      "source": [
        "def build_transformer():\n",
        "    transformer = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # mean, std, RGB값\n",
        "    ])\n",
        "    return transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc17fc04",
      "metadata": {
        "id": "cc17fc04"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for a, b in batch: \n",
        "        images.append(a)\n",
        "        targets.append(b)\n",
        "    images = torch.stack(images, dim=0) \n",
        "    targets = torch.stack(targets, dim=0)\n",
        "\n",
        "    return images, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1461af55",
      "metadata": {
        "id": "1461af55"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856b11e4",
      "metadata": {
        "id": "856b11e4"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./DATASET/Segmentation/\"\n",
        "transformer = build_transformer()\n",
        "tr_dataset = CT_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
        "val_dataset = CT_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
        "tr_dataloader = DataLoader(tr_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658e9069",
      "metadata": {
        "id": "658e9069"
      },
      "outputs": [],
      "source": [
        "def build_dataloader(data_dir, batch_size=4):\n",
        "    transformer = build_transformer()\n",
        "    \n",
        "    dataloaders = {}\n",
        "    train_dataset = CT_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
        "    dataloaders[\"train\"] = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    val_dataset = CT_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
        "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "    return dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a83e4c1",
      "metadata": {
        "id": "1a83e4c1"
      },
      "outputs": [],
      "source": [
        "for index, batch in enumerate(tr_dataloader):\n",
        "    images = batch[0]\n",
        "    targets = batch[1]\n",
        "    print(f\"images shape: {images.shape}\")\n",
        "    print(f\"masks shape: {targets.shape}\")\n",
        "    if index == 0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc7c3f9",
      "metadata": {
        "id": "2bc7c3f9"
      },
      "source": [
        "## 3. U-Net 데이터 아키텍처 구현해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61249f74",
      "metadata": {
        "id": "61249f74"
      },
      "source": [
        "![unet-architecture.png](attachment:unet-architecture.png)\n",
        "그림 출처: https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b15b7c6",
      "metadata": {
        "id": "8b15b7c6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3cab31d",
      "metadata": {
        "id": "b3cab31d"
      },
      "outputs": [],
      "source": [
        "def ConvLayer(in_channels, out_channels):\n",
        "    layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        \n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "    return layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e820e0b4",
      "metadata": {
        "id": "e820e0b4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_block1 = ConvLayer(in_channels=3, out_channels=64)\n",
        "        self.conv_block2 = ConvLayer(in_channels=64, out_channels=128)\n",
        "        self.conv_block3 = ConvLayer(in_channels=128, out_channels=256)\n",
        "        self.conv_block4 = ConvLayer(in_channels=256, out_channels=512)\n",
        "        self.conv_block5 = ConvLayer(in_channels=512, out_channels=1024)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        encode_features = []\n",
        "        out = self.conv_block1(x)\n",
        "        encode_features.append(out)\n",
        "        out = self.pool(out)\n",
        "        \n",
        "        out = self.conv_block2(out)\n",
        "        encode_features.append(out)\n",
        "        out = self.pool(out)\n",
        "        \n",
        "        out = self.conv_block3(out)\n",
        "        encode_features.append(out)\n",
        "        out = self.pool(out)\n",
        "        \n",
        "        out = self.conv_block4(out)\n",
        "        encode_features.append(out)\n",
        "        out = self.pool(out)\n",
        "        \n",
        "        out = self.conv_block5(out)\n",
        "        return out, encode_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b44878",
      "metadata": {
        "id": "12b44878"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out, ftrs = encoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fee28f",
      "metadata": {
        "id": "27fee28f"
      },
      "outputs": [],
      "source": [
        "for ftr in ftrs:\n",
        "    print(ftr.shape)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7ee447",
      "metadata": {
        "id": "4b7ee447"
      },
      "outputs": [],
      "source": [
        "def UpConvLayer(in_channels, out_channels):\n",
        "    layers = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d134a1",
      "metadata": {
        "id": "40d134a1"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.upconv_layer1 = UpConvLayer(in_channels=1024, out_channels=512)\n",
        "        self.conv_block1 = ConvLayer(in_channels=512+512, out_channels=512)\n",
        "        \n",
        "        self.upconv_layer2 = UpConvLayer(in_channels=512, out_channels=256)\n",
        "        self.conv_block2 = ConvLayer(in_channels=256+256, out_channels=256)\n",
        "        \n",
        "        self.upconv_layer3 = UpConvLayer(in_channels=256, out_channels=128)\n",
        "        self.conv_block3 = ConvLayer(in_channels=128+128, out_channels=128)\n",
        "        \n",
        "        self.upconv_layer4 = UpConvLayer(in_channels=128, out_channels=64)\n",
        "        self.conv_block4 = ConvLayer(in_channels=64+64, out_channels=64)\n",
        "        \n",
        "    def forward(self, x, encoder_features):\n",
        "        out = self.upconv_layer1(x)\n",
        "        croped_enc_feature = self._center_crop(encoder_features[-1], out.shape[2:])\n",
        "        out = torch.cat([out, croped_enc_feature], dim=1)\n",
        "        out = self.conv_block1(out)\n",
        "        \n",
        "        out = self.upconv_layer2(out)\n",
        "        croped_enc_feature = self._center_crop(encoder_features[-2], out.shape[2:])\n",
        "        out = torch.cat([out, croped_enc_feature], dim=1)\n",
        "        out = self.conv_block2(out)\n",
        "        \n",
        "        out = self.upconv_layer3(out)\n",
        "        croped_enc_feature = self._center_crop(encoder_features[-3], out.shape[2:])\n",
        "        out = torch.cat([out, croped_enc_feature], dim=1)\n",
        "        out = self.conv_block3(out)\n",
        "        \n",
        "        out = self.upconv_layer4(out)\n",
        "        croped_enc_feature = self._center_crop(encoder_features[-4], out.shape[2:])\n",
        "        out = torch.cat([out, croped_enc_feature], dim=1)\n",
        "        out = self.conv_block4(out)\n",
        "        return out\n",
        "        \n",
        "    def _center_crop(self, encoder_feature, decoder_feature_size):\n",
        "        croped_features = transforms.CenterCrop(size=decoder_feature_size)(encoder_feature)\n",
        "        return croped_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1be6706",
      "metadata": {
        "id": "e1be6706"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out, ftrs = encoder(x)\n",
        "out = decoder(out, ftrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9482fd",
      "metadata": {
        "id": "2e9482fd"
      },
      "outputs": [],
      "source": [
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9975221",
      "metadata": {
        "id": "d9975221"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10fbf4e7",
      "metadata": {
        "id": "10fbf4e7"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes, retain_input_dim=True):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.head = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        self.retain_input_dim = retain_input_dim\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out, encode_features = self.encoder(x)\n",
        "        out = self.decoder(out, encode_features)\n",
        "        out = self.head(out)\n",
        "        if self.retain_input_dim:\n",
        "            _, _, H, W = x.shape\n",
        "            out = F.interpolate(out, size=(H,W))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46aff9ef",
      "metadata": {
        "id": "46aff9ef"
      },
      "outputs": [],
      "source": [
        "model = UNet(num_classes=4)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out = model(x)\n",
        "print(f\"input shape: {x.shape}\")\n",
        "print(f\"output shape: {out.shape}\") # 4 = 백그라운드, 폐, 기도, 심장"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d0823c",
      "metadata": {
        "id": "11d0823c"
      },
      "source": [
        "## 4. Dice similarity coefficient 설명 및 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4e7f56",
      "metadata": {
        "id": "bf4e7f56"
      },
      "source": [
        "![Scheme-to-explain-how-Dice-coefficient-is-calculated-The-light-red-and-light-green.png](attachment:Scheme-to-explain-how-Dice-coefficient-is-calculated-The-light-red-and-light-green.png)\n",
        "그림 출처: https://www.researchgate.net/figure/Scheme-to-explain-how-Dice-coefficient-is-calculated-The-light-red-and-light-green_fig4_352895635"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1fe0f1",
      "metadata": {
        "id": "1b1fe0f1"
      },
      "source": [
        "### Dice similarity coefficient(DSC) == F1 score? YES only if binary segmentation task!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e44b8e",
      "metadata": {
        "id": "34e44b8e"
      },
      "source": [
        "![JCSB-07-209-g003.gif](attachment:JCSB-07-209-g003.gif)\n",
        "그림 출처: https://www.omicsonline.org/articles-images/JCSB-07-209-g003.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b41b0e9a",
      "metadata": {
        "id": "b41b0e9a"
      },
      "source": [
        "### What is Dice coefficient loss or Dice loss? Dice loss = 1 - DSC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285419af",
      "metadata": {
        "id": "285419af"
      },
      "outputs": [],
      "source": [
        "for index, batch in enumerate(tr_dataloader):\n",
        "    images = batch[0]\n",
        "    targets = batch[1]\n",
        "    predictions = model(images)\n",
        "    \n",
        "    if index == 0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab761a33",
      "metadata": {
        "id": "ab761a33"
      },
      "outputs": [],
      "source": [
        "num_classes = 4\n",
        "\n",
        "predictions_ = torch.argmax(predictions, dim=1)\n",
        "onehot_pred = F.one_hot(predictions_, num_classes=num_classes).permute(0, 3, 1, 2)\n",
        "onehot_target = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef26837",
      "metadata": {
        "id": "aef26837"
      },
      "outputs": [],
      "source": [
        "onehot_pred_ = onehot_pred[0]\n",
        "onehot_target_ = onehot_target[0]\n",
        "\n",
        "dice_coeff = 0\n",
        "for class_index in range(1, num_classes):\n",
        "    a = onehot_pred_[class_index]\n",
        "    b = onehot_target_[class_index]\n",
        "    set_inter = torch.dot(a.reshape(-1).float(), b.reshape(-1).float())\n",
        "    set_sum = a.sum() + b.sum()\n",
        "    dice_coeff += (2 * set_inter) / (set_sum + 1e-9)\n",
        "dice_coeff /= (num_classes-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41592e9",
      "metadata": {
        "id": "b41592e9"
      },
      "outputs": [],
      "source": [
        "dice_loss = 1. - dice_coeff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b0ec69",
      "metadata": {
        "id": "c7b0ec69"
      },
      "outputs": [],
      "source": [
        "class UNet_metric():\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "    def __call__(self, pred, target):\n",
        "        onehot_pred = F.one_hot(torch.argmax(pred, dim=1), num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
        "        onehot_target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
        "        dice_loss = self._get_dice_loss(onehot_pred, onehot_target)\n",
        "        dice_coefficient = self._get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
        "        return dice_loss, dice_coefficient\n",
        "    \n",
        "    def _get_dice_coeffient(self, pred, target):\n",
        "        set_inter = torch.dot(pred.reshape(-1).float(), target.reshape(-1).float())\n",
        "        set_sum = pred.sum() + target.sum()\n",
        "        if set_sum.item() == 0:\n",
        "            set_sum = 2 * set_inter\n",
        "        dice_coeff = (2 * set_inter) / (set_sum + 1e-9)\n",
        "        return dice_coeff\n",
        "    \n",
        "    def _get_multiclass_dice_coefficient(self, pred, target):\n",
        "        dice = 0\n",
        "        for class_index in range(1, self.num_classes):\n",
        "            dice += self._get_dice_coeffient(pred[class_index], target[class_index])\n",
        "        return dice / (self.num_classes - 1)\n",
        "    \n",
        "    def _get_batch_dice_coefficient(self, pred, target):\n",
        "        num_batch = pred.shape[0]\n",
        "        dice = 0\n",
        "        for batch_index in range(num_batch):\n",
        "            dice += self._get_multiclass_dice_coefficient(pred[batch_index], target[batch_index])\n",
        "        return dice / num_batch\n",
        "    \n",
        "    def _get_dice_loss(self, pred, target):\n",
        "        return 1 - self._get_batch_dice_coefficient(pred, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e756672a",
      "metadata": {
        "id": "e756672a"
      },
      "outputs": [],
      "source": [
        "criterion = UNet_metric(num_classes=4)\n",
        "criterion(predictions, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935191b5",
      "metadata": {
        "id": "935191b5"
      },
      "source": [
        "## 5. Loss function 구현 및 SGDM 최적화 적용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f05e88",
      "metadata": {
        "id": "d4f05e88"
      },
      "outputs": [],
      "source": [
        "class UNet_metric():\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "        \n",
        "    def __call__(self, pred, target):\n",
        "        loss1 = self.CE_loss(pred, target)\n",
        "        onehot_pred = F.one_hot(torch.argmax(pred, dim=1), num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
        "        onehot_target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
        "        loss2 = self._get_dice_loss(onehot_pred, onehot_target)\n",
        "        loss = loss1 + loss2\n",
        "        \n",
        "        dice_coefficient = self._get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
        "        return loss, dice_coefficient\n",
        "    \n",
        "    def _get_dice_coeffient(self, pred, target):\n",
        "        set_inter = torch.dot(pred.reshape(-1).float(), target.reshape(-1).float())\n",
        "        set_sum = pred.sum() + target.sum()\n",
        "        if set_sum.item() == 0:\n",
        "            set_sum = 2 * set_inter\n",
        "        dice_coeff = (2 * set_inter) / (set_sum + 1e-9)\n",
        "        return dice_coeff\n",
        "    \n",
        "    def _get_multiclass_dice_coefficient(self, pred, target):\n",
        "        dice = 0\n",
        "        for class_index in range(1, self.num_classes):\n",
        "            dice += self._get_dice_coeffient(pred[class_index], target[class_index])\n",
        "        return dice / (self.num_classes - 1)\n",
        "    \n",
        "    def _get_batch_dice_coefficient(self, pred, target):\n",
        "        num_batch = pred.shape[0]\n",
        "        dice = 0\n",
        "        for batch_index in range(num_batch):\n",
        "            dice += self._get_multiclass_dice_coefficient(pred[batch_index], target[batch_index])\n",
        "        return dice / num_batch\n",
        "    \n",
        "    def _get_dice_loss(self, pred, target):\n",
        "        return 1 - self._get_batch_dice_coefficient(pred, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021cf67c",
      "metadata": {
        "id": "021cf67c"
      },
      "outputs": [],
      "source": [
        "criterion = UNet_metric(num_classes=4)\n",
        "criterion(predictions, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a72460",
      "metadata": {
        "id": "62a72460"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr= 1E-3, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01752f0",
      "metadata": {
        "id": "f01752f0"
      },
      "outputs": [],
      "source": [
        "for index, batch in enumerate(tr_dataloader):\n",
        "    images = batch[0]\n",
        "    targets = batch[1]\n",
        "    predictions = model(images)\n",
        "    criterion(predictions, targets)\n",
        "    if index == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03c1455",
      "metadata": {
        "id": "a03c1455"
      },
      "source": [
        "## 6. Semantic segmentation 학습을 위한 코드 작성하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ac1fe6",
      "metadata": {
        "id": "24ac1fe6"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(dataloaders, model, optimizer, criterion, device):\n",
        "    losses = {}\n",
        "    dice_coefficients = {}\n",
        "    \n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        running_loss = 0.0\n",
        "        running_dice_coeff = 0.0\n",
        "        \n",
        "        if phase == \"train\":\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        \n",
        "        for index, batch in enumerate(dataloaders[phase]):\n",
        "            images = batch[0].to(device)\n",
        "            targets = batch[1].to(device)\n",
        "            \n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                predictions = model(images)\n",
        "                loss, dice_coefficient = criterion(predictions, targets)\n",
        "                \n",
        "            if phase == \"train\":\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            running_dice_coeff += dice_coefficient.item()\n",
        "            \n",
        "            if phase == \"train\":\n",
        "                if index % 100 == 0:\n",
        "                    text = f\"{index}/{len(dataloaders[phase])}\" + \\\n",
        "                            f\" - Running Loss: {loss.item():.4f}\" + \\\n",
        "                            f\" - Running Dice: {dice_coefficient.item():.4f}\" \n",
        "                    print(text)\n",
        "\n",
        "        losses[phase] = running_loss / len(dataloaders[phase])\n",
        "        dice_coefficients[phase] = running_dice_coeff / len(dataloaders[phase])\n",
        "    return losses, dice_coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c9af34",
      "metadata": {
        "id": "44c9af34"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4c88f1",
      "metadata": {
        "id": "0b4c88f1"
      },
      "outputs": [],
      "source": [
        "from utils import save_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0d3716",
      "metadata": {
        "id": "7e0d3716",
        "outputId": "34db9f15-6056-4a40-a73c-98426cbee099"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-58-dbe89e6c08ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUNet_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1E-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    905\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m     def register_backward_hook(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 905\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# are found or any other error occurs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;31m# we need to just return without initializing in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "data_dir = \"./DATASET/Segmentation/\"\n",
        "is_cuda = True\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "IMAGE_SIZE = 224\n",
        "# BATCH_SIZE = 2\n",
        "BATCH_SIZE = 12\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
        "\n",
        "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
        "model = UNet(num_classes=NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= 1E-3, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1afb8d",
      "metadata": {
        "scrolled": true,
        "id": "4a1afb8d",
        "outputId": "343d945a-3e0f-436f-e2b5-d510a89a2608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0/1243 - Running Loss: 2.5785 - Running Dice: 0.0108\n",
            "100/1243 - Running Loss: 1.4298 - Running Dice: 0.0000\n",
            "200/1243 - Running Loss: 1.2178 - Running Dice: 0.1283\n",
            "300/1243 - Running Loss: 0.9955 - Running Dice: 0.1424\n",
            "400/1243 - Running Loss: 0.9673 - Running Dice: 0.1605\n",
            "500/1243 - Running Loss: 0.9038 - Running Dice: 0.2061\n",
            "600/1243 - Running Loss: 1.1584 - Running Dice: 0.1348\n",
            "700/1243 - Running Loss: 0.9321 - Running Dice: 0.2306\n",
            "800/1243 - Running Loss: 0.9693 - Running Dice: 0.1576\n",
            "900/1243 - Running Loss: 0.9123 - Running Dice: 0.2033\n",
            "1000/1243 - Running Loss: 0.9303 - Running Dice: 0.2025\n",
            "1100/1243 - Running Loss: 0.8946 - Running Dice: 0.1956\n",
            "1200/1243 - Running Loss: 0.8755 - Running Dice: 0.2566\n",
            "0/10 - Train Loss: 1.0428, Val Loss: 0.9351\n",
            "0/10 - Train Dice Coeff: 0.1576, Val Dice Coeff: 0.2024\n",
            "0/1243 - Running Loss: 0.9468 - Running Dice: 0.1566\n",
            "100/1243 - Running Loss: 1.0464 - Running Dice: 0.1615\n",
            "200/1243 - Running Loss: 0.9562 - Running Dice: 0.2028\n",
            "300/1243 - Running Loss: 0.8902 - Running Dice: 0.1759\n",
            "400/1243 - Running Loss: 0.9950 - Running Dice: 0.1749\n",
            "500/1243 - Running Loss: 0.8050 - Running Dice: 0.2540\n",
            "600/1243 - Running Loss: 0.9045 - Running Dice: 0.2111\n",
            "700/1243 - Running Loss: 0.9449 - Running Dice: 0.1111\n",
            "800/1243 - Running Loss: 0.7987 - Running Dice: 0.2598\n",
            "900/1243 - Running Loss: 0.8962 - Running Dice: 0.2440\n",
            "1000/1243 - Running Loss: 0.8291 - Running Dice: 0.2404\n",
            "1100/1243 - Running Loss: 0.7637 - Running Dice: 0.2988\n",
            "1200/1243 - Running Loss: 0.7936 - Running Dice: 0.3046\n",
            "1/10 - Train Loss: 0.8668, Val Loss: 0.8967\n",
            "1/10 - Train Dice Coeff: 0.2213, Val Dice Coeff: 0.2590\n",
            "0/1243 - Running Loss: 0.7974 - Running Dice: 0.2386\n",
            "100/1243 - Running Loss: 0.8298 - Running Dice: 0.2336\n",
            "200/1243 - Running Loss: 0.7756 - Running Dice: 0.2688\n",
            "300/1243 - Running Loss: 0.8626 - Running Dice: 0.2164\n",
            "400/1243 - Running Loss: 0.7728 - Running Dice: 0.2769\n",
            "500/1243 - Running Loss: 0.7685 - Running Dice: 0.2995\n",
            "600/1243 - Running Loss: 0.8277 - Running Dice: 0.2135\n",
            "700/1243 - Running Loss: 0.7711 - Running Dice: 0.2982\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [127]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m val_loss, val_dice_coefficient \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 9\u001b[0m     losses, dice_coefficients \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     11\u001b[0m     val_loss\u001b[38;5;241m.\u001b[39mappend(losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(dataloaders, model, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 20\u001b[0m     loss, dice_coefficient \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     23\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36mUNet_metric.__call__\u001b[1;34m(self, pred, target)\u001b[0m\n\u001b[0;32m      8\u001b[0m onehot_pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      9\u001b[0m onehot_target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(target, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dice_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43monehot_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monehot_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2\n\u001b[0;32m     13\u001b[0m dice_coefficient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
            "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36mUNet_metric._get_dice_loss\u001b[1;34m(self, pred, target)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_dice_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred, target):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batch_dice_coefficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36mUNet_metric._get_batch_dice_coefficient\u001b[1;34m(self, pred, target)\u001b[0m\n\u001b[0;32m     32\u001b[0m dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batch):\n\u001b[1;32m---> 34\u001b[0m     dice \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_multiclass_dice_coefficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dice \u001b[38;5;241m/\u001b[39m num_batch\n",
            "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36mUNet_metric._get_multiclass_dice_coefficient\u001b[1;34m(self, pred, target)\u001b[0m\n\u001b[0;32m     25\u001b[0m dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes):\n\u001b[1;32m---> 27\u001b[0m     dice \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dice_coeffient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dice \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
            "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36mUNet_metric._get_dice_coeffient\u001b[1;34m(self, pred, target)\u001b[0m\n\u001b[0;32m     17\u001b[0m set_inter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     18\u001b[0m set_sum \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m target\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mset_sum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     20\u001b[0m     set_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m set_inter\n\u001b[0;32m     21\u001b[0m dice_coeff \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m set_inter) \u001b[38;5;241m/\u001b[39m (set_sum \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-9\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "best_epoch = 0\n",
        "best_score = 0.0\n",
        "train_loss, train_dice_coefficient = [], []\n",
        "val_loss, val_dice_coefficient = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    losses, dice_coefficients = train_one_epoch(dataloaders, model, optimizer, criterion, DEVICE)\n",
        "    train_loss.append(losses[\"train\"])\n",
        "    val_loss.append(losses[\"val\"])\n",
        "    train_dice_coefficient.append(dice_coefficients[\"train\"])\n",
        "    val_dice_coefficient.append(dice_coefficients[\"val\"])\n",
        "    \n",
        "    print(f\"{epoch}/{num_epochs} - Train Loss: {losses['train']:.4f}, Val Loss: {losses['val']:.4f}\")\n",
        "    print(f\"{epoch}/{num_epochs} - Train Dice Coeff: {dice_coefficients['train']:.4f}, Val Dice Coeff: {dice_coefficients['val']:.4f}\")\n",
        "    \n",
        "    if (epoch > 3) and (dice_coefficients[\"val\"] > best_score):\n",
        "        best_epoch = epoch\n",
        "        best_score = dice_coefficients[\"val\"]\n",
        "        save_model(model.state_dict(), f\"model_{epoch:02d}.pth\")\n",
        "        \n",
        "print(f\"Best epoch: {best_epoch} -> Best Dice Coeffient: {best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030a866c",
      "metadata": {
        "id": "030a866c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 5))\n",
        "plt.subplot(211)\n",
        "plt.plot(train_loss, label=\"train\")\n",
        "plt.plot(val_loss,  label=\"val\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.grid(\"on\")\n",
        "plt.legend()\n",
        "plt.subplot(212)\n",
        "plt.plot(train_dice_coefficient, label=\"train\")\n",
        "plt.plot(val_dice_coefficient, label=\"val\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"dice coefficient\")\n",
        "plt.grid(\"on\")\n",
        "plt.legend()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef5896b",
      "metadata": {
        "id": "9ef5896b"
      },
      "source": [
        "![loss_figure.png](attachment:loss_figure.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420bccdb",
      "metadata": {
        "id": "420bccdb"
      },
      "source": [
        "## 7. 모델 테스트 및 Morphological filtering 적용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5356a19",
      "metadata": {
        "id": "d5356a19"
      },
      "outputs": [],
      "source": [
        "def load_model(ckpt_path, num_classes, device):\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    model = UNet(num_classes=num_classes)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e33ad6",
      "metadata": {
        "id": "f1e33ad6"
      },
      "outputs": [],
      "source": [
        "is_cuda = True\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "IMAGE_SIZE = 224\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5482ac8e",
      "metadata": {
        "id": "5482ac8e"
      },
      "outputs": [],
      "source": [
        "ckpt_path = \"./trained_model/model_05.pth\"\n",
        "model = load_model(ckpt_path, NUM_CLASSES, DEVICE)\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ec3bd5",
      "metadata": {
        "id": "c4ec3bd5"
      },
      "outputs": [],
      "source": [
        "def morpholocal_process(mask, num_classes, ksize=7):\n",
        "    new_mask = mask.copy()\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(ksize, ksize))\n",
        "\n",
        "    for class_index in range(1, num_classes):\n",
        "        binary_mask = (mask == class_index).astype(np.uint8)\n",
        "        closing = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
        "        new_mask[closing.astype(np.bool_)] = class_index\n",
        "    return new_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305b2092",
      "metadata": {
        "id": "305b2092"
      },
      "outputs": [],
      "source": [
        "from utils import CLASS_ID_TO_RGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff2438f",
      "metadata": {
        "id": "cff2438f"
      },
      "outputs": [],
      "source": [
        "def decode_segmap(mask, num_classes):\n",
        "    mask_H, mask_W = mask.shape\n",
        "    R_channel = np.zeros((mask_H, mask_W), dtype=np.uint8)\n",
        "    G_channel = np.zeros((mask_H, mask_W), dtype=np.uint8)\n",
        "    B_channel = np.zeros((mask_H, mask_W), dtype=np.uint8)\n",
        "\n",
        "    for class_index in range(1, num_classes):\n",
        "        R_channel[mask == class_index] = CLASS_ID_TO_RGB[class_index][0]\n",
        "        G_channel[mask == class_index] = CLASS_ID_TO_RGB[class_index][1]\n",
        "        B_channel[mask == class_index] = CLASS_ID_TO_RGB[class_index][2]\n",
        "    \n",
        "    RGB_mask = cv2.merge((B_channel, G_channel, R_channel))\n",
        "    return RGB_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7528e44",
      "metadata": {
        "id": "f7528e44"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2249d57b",
      "metadata": {
        "id": "2249d57b"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_segment(image, model, num_classes, device):\n",
        "    PIL_image = Image.fromarray(image)\n",
        "    tensor_image = transformer(PIL_image)\n",
        "    tensor_image = tensor_image.to(device)\n",
        "\n",
        "    pred_mask = model(torch.unsqueeze(tensor_image, dim=0))\n",
        "    pred_mask = torch.argmax(pred_mask.squeeze(0).cpu(), dim=0)\n",
        "    pred_mask = pred_mask.numpy()\n",
        "    pred_mask = morpholocal_process(pred_mask, num_classes)\n",
        "    rgb_mask = decode_segmap(pred_mask, num_classes)\n",
        "    return rgb_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3894b1",
      "metadata": {
        "id": "ea3894b1"
      },
      "outputs": [],
      "source": [
        "video_path = \"./DATASET/Segmentation/video/test/ID00411637202309374271828.mp4\"\n",
        "cnt = 0\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "while (vidcap.isOpened()):\n",
        "    ret, frame = vidcap.read()\n",
        "\n",
        "    if ret:\n",
        "        rgb_mask = predict_segment(frame, model, NUM_CLASSES, DEVICE)\n",
        "        rgb_mask = cv2.resize(rgb_mask, dsize=frame.shape[:2])\n",
        "              \n",
        "        alpha = 0.6\n",
        "        blend = cv2.addWeighted(frame, alpha, rgb_mask, 1-alpha, 0)\n",
        "        cv2.imshow('output', blend)\n",
        "        \n",
        "        key = cv2.waitKey(1)\n",
        "        if key == 27:\n",
        "            break\n",
        "        if key == ord('s'):\n",
        "            cv2.waitKey(0)\n",
        "    else:\n",
        "        break\n",
        "        \n",
        "vidcap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316c9b82",
      "metadata": {
        "id": "316c9b82"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Segmentation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}